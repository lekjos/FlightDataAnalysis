{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This notebook contains the code to create a database in postgres and import flight data. In produciton, I normally would not use a Juypter notebook, but it makes it easy to present my code in this demo.\n",
    "\n",
    "The first cell below contains all our necessary imports and loads our environment variables. Be sure to run the cells in order or press the run all button up top, otherwise you may get errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\leifk\\Documents\\GitHub\\FlightDataAnalysis\n",
      "Data Directory: C:\\Users\\Public\\data\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, datetime\n",
    "from DBConn import DBConn\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.dialects.postgresql import DOUBLE_PRECISION, SMALLINT, BOOLEAN, VARCHAR, CHAR\n",
    "from sqlalchemy import create_engine, event\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.realpath(\"__file__\")) #Change if needed, defaults to location of this file\n",
    "print('Project Root:', PROJECT_ROOT)\n",
    "# DATA_DIR = os.path.join(PROJECT_ROOT,'data') # modify as needed or move data to projec_root\\data\\ (which is in .gitignore)\n",
    "DATA_DIR = 'C:\\\\Users\\\\Public\\\\data\\\\' # modify as needed or move data to projec_root\\data\\ (which is in .gitignore)\n",
    "print('Data Directory:', DATA_DIR)\n",
    "DATABASE_NAME = 'flight_data'\n",
    "DBConn.set_database(DATABASE_NAME)\n",
    "\n",
    "# store data paths for later use -- modify filenames if yours are different\n",
    "FLIGHT_DATA_DIR = os.path.join(DATA_DIR, 'FlightDataUncompressed')\n",
    "FLIGHT_DATA_FNAMES = os.listdir(FLIGHT_DATA_DIR)\n",
    "FLIGHT_DATA_PATHS = []\n",
    "for file_name in FLIGHT_DATA_FNAMES:\n",
    "    FLIGHT_DATA_PATHS.append(os.path.join(FLIGHT_DATA_DIR, file_name))\n",
    "AIRPORTS_PATH = os.path.join(DATA_DIR, 'airports.csv')\n",
    "CARRIERS_PATH = os.path.join(DATA_DIR, 'carriers.csv')\n",
    "\n",
    "# load enviornment variables storing database conneciton info\n",
    "load_dotenv(PROJECT_ROOT)   #values can now be accessed using os.getenv(KEY_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA with Pandas -- determine structure of data for database schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.csv: 1311826\n",
      "1988.csv: 5202096\n",
      "1989.csv: 5041200\n",
      "1990.csv: 5270893\n",
      "1991.csv: 5076925\n",
      "1992.csv: 5092157\n",
      "1993.csv: 5070501\n",
      "1994.csv: 5180048\n",
      "1995.csv: 5327435\n",
      "1996.csv: 5351983\n",
      "1997.csv: 5411843\n",
      "1998.csv: 5384721\n",
      "1999.csv: 5527884\n",
      "2000.csv: 5683047\n",
      "2001.csv: 5967780\n",
      "2002.csv: 5271359\n",
      "2003.csv: 6488540\n",
      "2004.csv: 7129270\n",
      "2005.csv: 7140596\n",
      "2006.csv: 7141922\n",
      "2007.csv: 7453215\n",
      "2008.csv: 7009728\n",
      "total rows: 123534969\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "def row_count(f):\n",
    "\n",
    "    for i, l in enumerate(f):\n",
    "        pass\n",
    "    return i   \n",
    "\n",
    "total_rows = 0\n",
    "file_rows = dict()\n",
    "for path_ in FLIGHT_DATA_PATHS:\n",
    "    name = os.path.basename(path_)\n",
    "    with open(path_,'rb') as file:\n",
    "        if int(name.rstrip('.csv'))>=0:       \n",
    "            file_rows[name] = row_count(file)\n",
    "            total_rows += file_rows[name]\n",
    "            print(f\"{name}: {file_rows[name]}\")\n",
    "            # print(f'{name} chardet:')\n",
    "            # print(chardet.detect(file.read(1000)))\n",
    "print(f'total rows: {total_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 years of flight data starting: from 1987 - 2008\n",
      "looking at 1987\n"
     ]
    }
   ],
   "source": [
    "# ingest to pandas\n",
    "airports_df = pd.read_csv(AIRPORTS_PATH) \n",
    "carriers_df = pd.read_csv(CARRIERS_PATH)\n",
    "carriers_df = carriers_df.fillna('NA') ## Added after EDA, due to North American Airlines being interpreted as null\n",
    "\n",
    "print(f'Found {len(FLIGHT_DATA_FNAMES)} years of flight data starting: from {FLIGHT_DATA_FNAMES[0].rstrip(\".csv\")} - {FLIGHT_DATA_FNAMES[-1].rstrip(\".csv\")}')\n",
    "print(f'looking at {FLIGHT_DATA_FNAMES[0].rstrip(\".csv\")}')\n",
    "flight_df = pd.read_csv(FLIGHT_DATA_PATHS[9], encoding='ascii')\n",
    "\n",
    "def print_max_str_len(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print max length of string columns in pandas dataframe\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for col_name, col in df.iteritems():\n",
    "        try:\n",
    "            print(index, col_name, 'str', col.str.len().max())\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                print(index, col_name, 'int/float', max(col.map(str).apply(len)))\n",
    "            except AttributeError:\n",
    "                #lazy way to print col length of strings and ints\n",
    "                pass\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [iata, airport, city, state, country, lat, long]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "## Airports Schema\n",
    "# print(airports_df.head(5))\n",
    "print(airports_df[airports_df['iata'].astype(str).str.contains('CBM')])\n",
    "# print(airports_df.info())\n",
    "# print_max_str_len(airports_df)\n",
    "\n",
    "# print(airports_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Figure out Carriers Schema\n",
    "## uncomment lines to see what steps I took.\n",
    "\n",
    "# carriers_df.head()\n",
    "# carriers_df.info()\n",
    "# print_max_str_len(carriers_df)\n",
    "\n",
    "\n",
    "# print('null values:')\n",
    "# print(carriers_df.isnull().sum())\n",
    "# ## looks like we have a null value\n",
    "# carriers_df[carriers_df.Code.isnull()]\n",
    "# ##and it's caused by North American being interpreted as NA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flights Schema\n",
    "\n",
    "# flight_df.info()\n",
    "# flight_df.UniqueCarrier.isnull().sum()\n",
    "# print_max_str_len(flight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>2039.0</td>\n",
       "      <td>1930</td>\n",
       "      <td>2245.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>DL</td>\n",
       "      <td>345</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1931.0</td>\n",
       "      <td>1930</td>\n",
       "      <td>2142.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>DL</td>\n",
       "      <td>345</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>1930</td>\n",
       "      <td>2231.0</td>\n",
       "      <td>2139</td>\n",
       "      <td>DL</td>\n",
       "      <td>345</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>1550</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>1745</td>\n",
       "      <td>DL</td>\n",
       "      <td>411</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>1550</td>\n",
       "      <td>1841.0</td>\n",
       "      <td>1745</td>\n",
       "      <td>DL</td>\n",
       "      <td>411</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "0  1996      1          29          1   2039.0        1930   2245.0   \n",
       "1  1996      1          30          2   1931.0        1930   2142.0   \n",
       "2  1996      1          31          3   1956.0        1930   2231.0   \n",
       "3  1996      1           1          1   1730.0        1550   1909.0   \n",
       "4  1996      1           2          2   1714.0        1550   1841.0   \n",
       "\n",
       "   CRSArrTime UniqueCarrier  FlightNum  ... TaxiIn  TaxiOut  Cancelled  \\\n",
       "0        2139            DL        345  ...      6       10          0   \n",
       "1        2139            DL        345  ...      5       22          0   \n",
       "2        2139            DL        345  ...      7       27          0   \n",
       "3        1745            DL        411  ...      4       14          0   \n",
       "4        1745            DL        411  ...      4        8          0   \n",
       "\n",
       "   CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
       "0               NaN         0           NaN          NaN      NaN   \n",
       "1               NaN         0           NaN          NaN      NaN   \n",
       "2               NaN         0           NaN          NaN      NaN   \n",
       "3               NaN         0           NaN          NaN      NaN   \n",
       "4               NaN         0           NaN          NaN      NaN   \n",
       "\n",
       "   SecurityDelay  LateAircraftDelay  \n",
       "0            NaN                NaN  \n",
       "1            NaN                NaN  \n",
       "2            NaN                NaN  \n",
       "3            NaN                NaN  \n",
       "4            NaN                NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## More on flights\n",
    "\n",
    "# flight_df.UniqueCarrier[0]\n",
    "flight_df.head() # uncomment to see first five rows. You'll also need to comment next line.\n",
    "# print(flight_df.Diverted.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've now looked at the columns in each dataset and can see that we'll need three tables relaations between:\n",
    "- `flight.Origin`, `flight.Destination` to `airports.iata`\n",
    "- `flight.UniqueCarrier` to `carrier.Code`\n",
    "\n",
    "Now lets get the tables created in SQL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Flight Data Database\n",
    "The database in the .env file is just an entry point. We are going to now create new database and load in our flight data.\n",
    "\n",
    "#### Create database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Database\n",
    "\n",
    "\n",
    "# Create database if it doesn't exist already\n",
    "sql = f'''--sql\n",
    "SELECT 'CREATE DATABASE {DATABASE_NAME}'\n",
    "WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = '{DATABASE_NAME}');\n",
    "'''\n",
    "\n",
    "conn = DBConn()\n",
    "result = conn.exec(sql)\n",
    "print(\"Database created successfully or already exists....\")\n",
    "DBConn.set_database(DATABASE_NAME)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Airport Table\n",
    "conn = DBConn()\n",
    "print('accessing', conn.get_database())\n",
    "sql = f'''--sql\n",
    "--DROP TABLE flights;\n",
    "--DROP TABLE carriers;\n",
    "--DROP TABLE airports;\n",
    "\n",
    "--sql --comment included for code highlighting in IDE\n",
    " CREATE TABLE IF NOT EXISTS carriers (\n",
    "    \"Code\" VARCHAR(7) NOT NULL,\n",
    "    \"Description\" VARCHAR(100),\n",
    "    CONSTRAINT carriers_pkey PRIMARY KEY (\"Code\")\n",
    ");\n",
    "\n",
    "--sql \n",
    "CREATE TABLE IF NOT EXISTS airports (\n",
    "    iata VARCHAR(4) NOT NULL,\n",
    "    airport VARCHAR(50) NOT NULL,\n",
    "    city VARCHAR(40),\n",
    "    state CHAR(2),\n",
    "    country VARCHAR(40) NOT NULL,\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    long DOUBLE PRECISION NOT NULL,\n",
    "    CONSTRAINT airport_pkey PRIMARY KEY (iata)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS flights (\n",
    " \"Year\" SMALLINT NOT NULL,  \n",
    " \"Month\" SMALLINT NOT NULL,  \n",
    " \"DayofMonth\" SMALLINT NOT NULL,  \n",
    " \"DayOfWeek\" SMALLINT NOT NULL,  \n",
    " \"DepTime\" SMALLINT,\n",
    " \"CRSDepTime\" SMALLINT NOT NULL,  \n",
    " \"ArrTime\" SMALLINT,\n",
    " \"CRSArrTime\" SMALLINT NOT NULL,  \n",
    " \"UniqueCarrier\" VARCHAR(6) NOT NULL,\n",
    " \"FlightNum\" SMALLINT NOT NULL,  \n",
    " \"ActualElapsedTime\" SMALLINT,\n",
    " \"CRSElapsedTime\" SMALLINT,  \n",
    " \"AirTime\" SMALLINT,\n",
    " \"ArrDelay\" SMALLINT,\n",
    " \"DepDelay\" SMALLINT,\n",
    " \"Origin\" VARCHAR(3) NOT NULL, \n",
    " \"Dest\" VARCHAR(3) NOT NULL, \n",
    " \"Distance\" INTEGER,\n",
    " \"TaxiIn\" SMALLINT,\n",
    " \"TaxiOut\" SMALLINT,\n",
    " \"Cancelled\" BOOLEAN NOT NULL,  \n",
    " \"CarrierDelay\" SMALLINT,\n",
    " \"WeatherDelay\" SMALLINT,\n",
    " \"NASDelay\" SMALLINT,\n",
    " \"SecurityDelay\" SMALLINT,\n",
    " \"LateAircraftDelay\" SMALLINT,\n",
    " \n",
    " -- PRIMARY KEY (\"Year\", \"Month\", \"DayofMonth\", \"UniqueCarrier\", \"CRSDepTime\", \"FlightNum\", \"Origin\", \"CRSArrTime\"),\n",
    "\n",
    " CONSTRAINT origin_airport_fk\n",
    "    FOREIGN KEY (\"Origin\")\n",
    "    REFERENCES airports(iata)\n",
    "    ON DELETE CASCADE,\n",
    "\n",
    " CONSTRAINT dest_airport_fk \n",
    "    FOREIGN KEY (\"Dest\")\n",
    "    REFERENCES airports(iata)\n",
    "    ON DELETE CASCADE,\n",
    "\n",
    " CONSTRAINT uniquecarrier_carriers_fk \n",
    "    FOREIGN KEY (\"UniqueCarrier\")\n",
    "    REFERENCES carriers(\"Code\")\n",
    "    ON DELETE CASCADE\n",
    "\n",
    ");\n",
    "'''\n",
    "conn.exec(sql)\n",
    "print('create successful or already created')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports and Carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load carriers into databse\n",
    "\n",
    "load_carriers = False ## change to true to load\n",
    "if load_carriers:\n",
    "    engine = create_engine(\n",
    "        f'postgresql://{os.getenv(\"DB_USER\")}:{os.getenv(\"DB_PASS\")}@{os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\")}/{DATABASE_NAME}'\n",
    "    )\n",
    "    carriers_df.to_sql(\n",
    "        'carriers', \n",
    "        engine, \n",
    "        if_exists='append', \n",
    "        index=False, \n",
    "        dtype={\"Code\":VARCHAR, \"Description\": VARCHAR})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Airports into database\n",
    "load_airports = False ## change to true to load\n",
    "if load_airports:\n",
    "    engine = create_engine(\n",
    "        f'postgresql://{os.getenv(\"DB_USER\")}:{os.getenv(\"DB_PASS\")}@{os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\")}/{DATABASE_NAME}'\n",
    "    )\n",
    "    airports_df.to_sql(\n",
    "        'airports', \n",
    "        engine, \n",
    "        if_exists='append', \n",
    "        index=False, \n",
    "        dtype={\n",
    "            \"iata\": VARCHAR, \n",
    "            \"airport\": VARCHAR,\n",
    "            \"city\": VARCHAR,\n",
    "            \"state\": CHAR,\n",
    "            \"country\": VARCHAR,\n",
    "            \"lat\": DOUBLE_PRECISION(),\n",
    "            \"long\": DOUBLE_PRECISION(),\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading 2008.csv at 2022-01-22 18:14:28.884997\n",
      "working on chunk 0 at 2022-01-22 18:14:28.925007\n",
      "working on chunk 1 at 2022-01-22 18:14:37.040424\n",
      "working on chunk 2 at 2022-01-22 18:14:44.529860\n",
      "working on chunk 3 at 2022-01-22 18:14:52.424278\n",
      "working on chunk 4 at 2022-01-22 18:15:00.081595\n",
      "working on chunk 5 at 2022-01-22 18:15:07.639774\n",
      "working on chunk 6 at 2022-01-22 18:15:15.387070\n",
      "working on chunk 7 at 2022-01-22 18:15:23.318664\n",
      "working on chunk 8 at 2022-01-22 18:15:31.277152\n",
      "working on chunk 9 at 2022-01-22 18:15:39.084707\n",
      "working on chunk 10 at 2022-01-22 18:15:46.760302\n",
      "working on chunk 11 at 2022-01-22 18:15:54.777905\n",
      "working on chunk 12 at 2022-01-22 18:16:02.624210\n",
      "working on chunk 13 at 2022-01-22 18:16:10.346252\n",
      "working on chunk 14 at 2022-01-22 18:16:18.409987\n",
      "working on chunk 15 at 2022-01-22 18:16:26.362341\n",
      "working on chunk 16 at 2022-01-22 18:16:34.151862\n",
      "working on chunk 17 at 2022-01-22 18:16:41.979764\n",
      "working on chunk 18 at 2022-01-22 18:16:49.914284\n",
      "working on chunk 19 at 2022-01-22 18:16:57.473363\n",
      "working on chunk 20 at 2022-01-22 18:17:05.347494\n",
      "working on chunk 21 at 2022-01-22 18:17:13.250312\n",
      "working on chunk 22 at 2022-01-22 18:17:20.931344\n",
      "working on chunk 23 at 2022-01-22 18:17:28.846964\n",
      "working on chunk 24 at 2022-01-22 18:17:36.703938\n",
      "working on chunk 25 at 2022-01-22 18:17:44.519193\n",
      "working on chunk 26 at 2022-01-22 18:17:52.447235\n",
      "working on chunk 27 at 2022-01-22 18:18:00.285257\n",
      "working on chunk 28 at 2022-01-22 18:18:07.900370\n",
      "working on chunk 29 at 2022-01-22 18:18:15.767507\n",
      "working on chunk 30 at 2022-01-22 18:18:23.598631\n",
      "working on chunk 31 at 2022-01-22 18:18:31.503233\n",
      "working on chunk 32 at 2022-01-22 18:18:39.623582\n",
      "working on chunk 33 at 2022-01-22 18:18:47.464179\n",
      "working on chunk 34 at 2022-01-22 18:18:55.116672\n",
      "working on chunk 35 at 2022-01-22 18:19:03.119465\n",
      "working on chunk 36 at 2022-01-22 18:19:11.090181\n",
      "working on chunk 37 at 2022-01-22 18:19:18.890175\n",
      "working on chunk 38 at 2022-01-22 18:19:26.935878\n",
      "working on chunk 39 at 2022-01-22 18:19:34.863235\n",
      "working on chunk 40 at 2022-01-22 18:19:42.543462\n",
      "working on chunk 41 at 2022-01-22 18:19:50.374342\n",
      "working on chunk 42 at 2022-01-22 18:19:58.234268\n",
      "working on chunk 43 at 2022-01-22 18:20:05.894564\n",
      "working on chunk 44 at 2022-01-22 18:20:13.717114\n",
      "working on chunk 45 at 2022-01-22 18:20:21.577910\n",
      "working on chunk 46 at 2022-01-22 18:20:29.208236\n",
      "working on chunk 47 at 2022-01-22 18:20:37.089225\n",
      "working on chunk 48 at 2022-01-22 18:20:44.971556\n",
      "working on chunk 49 at 2022-01-22 18:20:52.591206\n",
      "working on chunk 50 at 2022-01-22 18:21:00.450986\n",
      "working on chunk 51 at 2022-01-22 18:21:08.274138\n",
      "working on chunk 52 at 2022-01-22 18:21:15.923556\n",
      "working on chunk 53 at 2022-01-22 18:21:23.767174\n",
      "working on chunk 54 at 2022-01-22 18:21:31.616187\n",
      "working on chunk 55 at 2022-01-22 18:21:39.319126\n",
      "working on chunk 56 at 2022-01-22 18:21:47.219257\n",
      "working on chunk 57 at 2022-01-22 18:21:55.219294\n",
      "working on chunk 58 at 2022-01-22 18:22:03.126863\n",
      "working on chunk 59 at 2022-01-22 18:22:10.944201\n",
      "working on chunk 60 at 2022-01-22 18:22:18.772163\n",
      "working on chunk 61 at 2022-01-22 18:22:26.372246\n",
      "working on chunk 62 at 2022-01-22 18:22:34.407167\n",
      "working on chunk 63 at 2022-01-22 18:22:42.242797\n",
      "working on chunk 64 at 2022-01-22 18:22:49.935192\n",
      "working on chunk 65 at 2022-01-22 18:22:57.795420\n",
      "working on chunk 66 at 2022-01-22 18:23:05.600461\n",
      "working on chunk 67 at 2022-01-22 18:23:13.265378\n",
      "working on chunk 68 at 2022-01-22 18:23:21.138265\n",
      "working on chunk 69 at 2022-01-22 18:23:28.948202\n",
      "working on chunk 70 at 2022-01-22 18:23:36.602221\n",
      "working on chunk 71 at 2022-01-22 18:23:44.582279\n",
      "working on chunk 72 at 2022-01-22 18:23:52.513727\n",
      "working on chunk 73 at 2022-01-22 18:24:00.211358\n",
      "working on chunk 74 at 2022-01-22 18:24:08.085157\n",
      "working on chunk 75 at 2022-01-22 18:24:15.941191\n",
      "working on chunk 76 at 2022-01-22 18:24:23.629939\n",
      "working on chunk 77 at 2022-01-22 18:24:31.532604\n",
      "working on chunk 78 at 2022-01-22 18:24:39.367876\n",
      "working on chunk 79 at 2022-01-22 18:24:47.042509\n",
      "working on chunk 80 at 2022-01-22 18:24:54.842388\n",
      "working on chunk 81 at 2022-01-22 18:25:02.718302\n",
      "working on chunk 82 at 2022-01-22 18:25:10.417201\n",
      "working on chunk 83 at 2022-01-22 18:25:18.334190\n",
      "working on chunk 84 at 2022-01-22 18:25:26.217107\n",
      "working on chunk 85 at 2022-01-22 18:25:34.011936\n",
      "working on chunk 86 at 2022-01-22 18:25:41.940841\n",
      "working on chunk 87 at 2022-01-22 18:25:49.788784\n",
      "working on chunk 88 at 2022-01-22 18:25:57.401761\n",
      "working on chunk 89 at 2022-01-22 18:26:05.290536\n",
      "working on chunk 90 at 2022-01-22 18:26:13.173572\n",
      "working on chunk 91 at 2022-01-22 18:26:20.779712\n",
      "working on chunk 92 at 2022-01-22 18:26:28.589612\n",
      "working on chunk 93 at 2022-01-22 18:26:36.389242\n",
      "working on chunk 94 at 2022-01-22 18:26:44.129289\n",
      "working on chunk 95 at 2022-01-22 18:26:51.927288\n",
      "working on chunk 96 at 2022-01-22 18:26:59.818775\n",
      "working on chunk 97 at 2022-01-22 18:27:07.465300\n",
      "working on chunk 98 at 2022-01-22 18:27:15.340471\n",
      "working on chunk 99 at 2022-01-22 18:27:23.176312\n",
      "working on chunk 100 at 2022-01-22 18:27:30.759019\n",
      "working on chunk 101 at 2022-01-22 18:27:38.576674\n",
      "working on chunk 102 at 2022-01-22 18:27:46.460813\n",
      "working on chunk 103 at 2022-01-22 18:27:54.106561\n",
      "working on chunk 104 at 2022-01-22 18:28:01.949051\n",
      "working on chunk 105 at 2022-01-22 18:28:09.778112\n",
      "working on chunk 106 at 2022-01-22 18:28:17.514765\n",
      "working on chunk 107 at 2022-01-22 18:28:25.400685\n",
      "working on chunk 108 at 2022-01-22 18:28:33.333336\n",
      "working on chunk 109 at 2022-01-22 18:28:40.998124\n",
      "working on chunk 110 at 2022-01-22 18:28:48.849282\n",
      "working on chunk 111 at 2022-01-22 18:28:56.650355\n",
      "working on chunk 112 at 2022-01-22 18:29:04.300107\n",
      "working on chunk 113 at 2022-01-22 18:29:12.138672\n",
      "working on chunk 114 at 2022-01-22 18:29:20.041480\n",
      "working on chunk 115 at 2022-01-22 18:29:27.709985\n",
      "working on chunk 116 at 2022-01-22 18:29:35.635483\n",
      "working on chunk 117 at 2022-01-22 18:29:43.126544\n",
      "working on chunk 118 at 2022-01-22 18:29:50.415053\n",
      "working on chunk 119 at 2022-01-22 18:29:57.878476\n",
      "working on chunk 120 at 2022-01-22 18:30:05.325291\n",
      "working on chunk 121 at 2022-01-22 18:30:12.530592\n",
      "working on chunk 122 at 2022-01-22 18:30:19.970440\n",
      "working on chunk 123 at 2022-01-22 18:30:27.339451\n",
      "working on chunk 124 at 2022-01-22 18:30:34.528912\n",
      "working on chunk 125 at 2022-01-22 18:30:41.879389\n",
      "working on chunk 126 at 2022-01-22 18:30:49.296007\n",
      "working on chunk 127 at 2022-01-22 18:30:56.516959\n",
      "working on chunk 128 at 2022-01-22 18:31:03.962433\n",
      "working on chunk 129 at 2022-01-22 18:31:11.341864\n",
      "working on chunk 130 at 2022-01-22 18:31:18.581986\n",
      "working on chunk 131 at 2022-01-22 18:31:26.055961\n",
      "working on chunk 132 at 2022-01-22 18:31:33.460355\n",
      "working on chunk 133 at 2022-01-22 18:31:40.803745\n",
      "working on chunk 134 at 2022-01-22 18:31:48.182789\n",
      "working on chunk 135 at 2022-01-22 18:31:55.599333\n",
      "working on chunk 136 at 2022-01-22 18:32:02.828237\n",
      "working on chunk 137 at 2022-01-22 18:32:10.174365\n",
      "working on chunk 138 at 2022-01-22 18:32:17.557385\n",
      "working on chunk 139 at 2022-01-22 18:32:24.732642\n",
      "working on chunk 140 at 2022-01-22 18:32:32.117747\n",
      "working on chunk 141 at 2022-01-22 18:32:39.500291\n",
      "working on chunk 142 at 2022-01-22 18:32:46.655318\n",
      "working on chunk 143 at 2022-01-22 18:32:54.036831\n",
      "working on chunk 144 at 2022-01-22 18:33:01.445587\n",
      "working on chunk 145 at 2022-01-22 18:33:08.680616\n",
      "working on chunk 146 at 2022-01-22 18:33:16.079190\n",
      "working on chunk 147 at 2022-01-22 18:33:23.585199\n",
      "working on chunk 148 at 2022-01-22 18:33:30.914333\n",
      "working on chunk 149 at 2022-01-22 18:33:38.282931\n",
      "working on chunk 150 at 2022-01-22 18:33:45.730463\n",
      "working on chunk 151 at 2022-01-22 18:33:53.052918\n",
      "working on chunk 152 at 2022-01-22 18:34:00.542843\n",
      "working on chunk 153 at 2022-01-22 18:34:08.016907\n",
      "working on chunk 154 at 2022-01-22 18:34:15.292446\n",
      "working on chunk 155 at 2022-01-22 18:34:22.680369\n",
      "working on chunk 156 at 2022-01-22 18:34:30.255546\n",
      "working on chunk 157 at 2022-01-22 18:34:37.645275\n",
      "working on chunk 158 at 2022-01-22 18:34:45.109419\n",
      "working on chunk 159 at 2022-01-22 18:34:52.459260\n",
      "working on chunk 160 at 2022-01-22 18:34:59.656743\n",
      "working on chunk 161 at 2022-01-22 18:35:07.071850\n",
      "working on chunk 162 at 2022-01-22 18:35:14.480683\n",
      "working on chunk 163 at 2022-01-22 18:35:21.650355\n",
      "working on chunk 164 at 2022-01-22 18:35:29.061715\n",
      "working on chunk 165 at 2022-01-22 18:35:36.527395\n",
      "working on chunk 166 at 2022-01-22 18:35:43.749165\n",
      "working on chunk 167 at 2022-01-22 18:35:51.210632\n",
      "working on chunk 168 at 2022-01-22 18:35:58.658276\n",
      "working on chunk 169 at 2022-01-22 18:36:05.801074\n",
      "working on chunk 170 at 2022-01-22 18:36:13.242385\n",
      "working on chunk 171 at 2022-01-22 18:36:20.705482\n",
      "working on chunk 172 at 2022-01-22 18:36:27.945748\n",
      "working on chunk 173 at 2022-01-22 18:36:35.413877\n",
      "working on chunk 174 at 2022-01-22 18:36:42.865349\n",
      "working on chunk 175 at 2022-01-22 18:36:50.077352\n",
      "working on chunk 176 at 2022-01-22 18:36:57.523614\n",
      "working on chunk 177 at 2022-01-22 18:37:04.957634\n",
      "working on chunk 178 at 2022-01-22 18:37:12.205527\n",
      "working on chunk 179 at 2022-01-22 18:37:19.769686\n",
      "working on chunk 180 at 2022-01-22 18:37:27.198736\n",
      "working on chunk 181 at 2022-01-22 18:37:34.401016\n",
      "working on chunk 182 at 2022-01-22 18:37:41.961635\n",
      "working on chunk 183 at 2022-01-22 18:37:49.321863\n",
      "working on chunk 184 at 2022-01-22 18:37:56.494431\n",
      "working on chunk 185 at 2022-01-22 18:38:03.818508\n",
      "working on chunk 186 at 2022-01-22 18:38:11.167579\n",
      "working on chunk 187 at 2022-01-22 18:38:18.372406\n",
      "working on chunk 188 at 2022-01-22 18:38:25.802644\n",
      "working on chunk 189 at 2022-01-22 18:38:33.158109\n",
      "working on chunk 190 at 2022-01-22 18:38:40.393925\n",
      "working on chunk 191 at 2022-01-22 18:38:47.815498\n",
      "working on chunk 192 at 2022-01-22 18:38:55.217316\n",
      "working on chunk 193 at 2022-01-22 18:39:02.466992\n",
      "working on chunk 194 at 2022-01-22 18:39:09.896781\n",
      "working on chunk 195 at 2022-01-22 18:39:17.315697\n",
      "working on chunk 196 at 2022-01-22 18:39:24.527955\n",
      "working on chunk 197 at 2022-01-22 18:39:31.990918\n",
      "working on chunk 198 at 2022-01-22 18:39:39.440418\n",
      "working on chunk 199 at 2022-01-22 18:39:46.658428\n",
      "working on chunk 200 at 2022-01-22 18:39:54.099146\n",
      "working on chunk 201 at 2022-01-22 18:40:01.501890\n",
      "working on chunk 202 at 2022-01-22 18:40:08.702747\n",
      "working on chunk 203 at 2022-01-22 18:40:16.158455\n",
      "working on chunk 204 at 2022-01-22 18:40:23.585440\n",
      "working on chunk 205 at 2022-01-22 18:40:30.813048\n",
      "working on chunk 206 at 2022-01-22 18:40:38.280609\n",
      "working on chunk 207 at 2022-01-22 18:40:45.851871\n",
      "working on chunk 208 at 2022-01-22 18:40:53.060739\n",
      "working on chunk 209 at 2022-01-22 18:41:00.483069\n",
      "working on chunk 210 at 2022-01-22 18:41:08.004313\n",
      "working on chunk 211 at 2022-01-22 18:41:15.239209\n",
      "working on chunk 212 at 2022-01-22 18:41:22.602697\n",
      "working on chunk 213 at 2022-01-22 18:41:30.057635\n",
      "working on chunk 214 at 2022-01-22 18:41:37.254589\n",
      "working on chunk 215 at 2022-01-22 18:41:44.607659\n",
      "working on chunk 216 at 2022-01-22 18:41:52.061533\n",
      "working on chunk 217 at 2022-01-22 18:41:59.411078\n",
      "working on chunk 218 at 2022-01-22 18:42:06.846230\n",
      "working on chunk 219 at 2022-01-22 18:42:14.202657\n",
      "working on chunk 220 at 2022-01-22 18:42:21.493779\n",
      "working on chunk 221 at 2022-01-22 18:42:29.156120\n",
      "working on chunk 222 at 2022-01-22 18:42:36.559689\n",
      "working on chunk 223 at 2022-01-22 18:42:43.774381\n",
      "working on chunk 224 at 2022-01-22 18:42:51.178191\n",
      "working on chunk 225 at 2022-01-22 18:42:58.603299\n",
      "working on chunk 226 at 2022-01-22 18:43:05.839539\n",
      "working on chunk 227 at 2022-01-22 18:43:13.213268\n",
      "working on chunk 228 at 2022-01-22 18:43:20.624990\n",
      "working on chunk 229 at 2022-01-22 18:43:27.853314\n",
      "working on chunk 230 at 2022-01-22 18:43:35.342979\n",
      "working on chunk 231 at 2022-01-22 18:43:42.834812\n",
      "working on chunk 232 at 2022-01-22 18:43:50.068678\n",
      "working on chunk 233 at 2022-01-22 18:43:57.482096\n",
      "working on chunk 234 at 2022-01-22 18:44:04.923071\n",
      "working on chunk 235 at 2022-01-22 18:44:12.203094\n",
      "working on chunk 236 at 2022-01-22 18:44:19.599054\n",
      "working on chunk 237 at 2022-01-22 18:44:26.990115\n",
      "working on chunk 238 at 2022-01-22 18:44:34.378860\n",
      "working on chunk 239 at 2022-01-22 18:44:41.836940\n",
      "working on chunk 240 at 2022-01-22 18:44:49.283996\n",
      "working on chunk 241 at 2022-01-22 18:44:56.467391\n",
      "working on chunk 242 at 2022-01-22 18:45:03.938718\n",
      "working on chunk 243 at 2022-01-22 18:45:11.315826\n",
      "working on chunk 244 at 2022-01-22 18:45:18.506187\n",
      "working on chunk 245 at 2022-01-22 18:45:25.901310\n",
      "working on chunk 246 at 2022-01-22 18:45:33.264880\n",
      "working on chunk 247 at 2022-01-22 18:45:40.528174\n",
      "working on chunk 248 at 2022-01-22 18:45:47.888698\n",
      "working on chunk 249 at 2022-01-22 18:45:55.237037\n",
      "working on chunk 250 at 2022-01-22 18:46:02.461546\n",
      "working on chunk 251 at 2022-01-22 18:46:09.840398\n",
      "working on chunk 252 at 2022-01-22 18:46:17.213029\n",
      "working on chunk 253 at 2022-01-22 18:46:24.392280\n",
      "working on chunk 254 at 2022-01-22 18:46:31.897276\n",
      "working on chunk 255 at 2022-01-22 18:46:39.297163\n",
      "working on chunk 256 at 2022-01-22 18:46:46.590963\n",
      "working on chunk 257 at 2022-01-22 18:46:54.001033\n",
      "working on chunk 258 at 2022-01-22 18:47:01.322404\n",
      "working on chunk 259 at 2022-01-22 18:47:08.561531\n",
      "working on chunk 260 at 2022-01-22 18:47:15.975071\n",
      "working on chunk 261 at 2022-01-22 18:47:23.370181\n",
      "working on chunk 262 at 2022-01-22 18:47:30.586022\n",
      "working on chunk 263 at 2022-01-22 18:47:37.966277\n",
      "working on chunk 264 at 2022-01-22 18:47:45.417026\n",
      "working on chunk 265 at 2022-01-22 18:47:52.558979\n",
      "working on chunk 266 at 2022-01-22 18:48:00.024125\n",
      "working on chunk 267 at 2022-01-22 18:48:07.514332\n",
      "working on chunk 268 at 2022-01-22 18:48:14.653663\n",
      "working on chunk 269 at 2022-01-22 18:48:22.079872\n",
      "working on chunk 270 at 2022-01-22 18:48:29.384579\n",
      "working on chunk 271 at 2022-01-22 18:48:36.624786\n",
      "working on chunk 272 at 2022-01-22 18:48:43.943989\n",
      "working on chunk 273 at 2022-01-22 18:48:51.326639\n",
      "working on chunk 274 at 2022-01-22 18:48:58.508853\n",
      "working on chunk 275 at 2022-01-22 18:49:05.909649\n",
      "working on chunk 276 at 2022-01-22 18:49:13.295811\n",
      "working on chunk 277 at 2022-01-22 18:49:20.532353\n",
      "working on chunk 278 at 2022-01-22 18:49:27.915634\n",
      "working on chunk 279 at 2022-01-22 18:49:35.310654\n",
      "working on chunk 280 at 2022-01-22 18:49:42.526293\n",
      "working on chunk 281 at 2022-01-22 18:49:50.436878\n",
      "working on chunk 282 at 2022-01-22 18:49:57.785078\n",
      "working on chunk 283 at 2022-01-22 18:50:05.074015\n",
      "working on chunk 284 at 2022-01-22 18:50:12.478785\n",
      "working on chunk 285 at 2022-01-22 18:50:19.887155\n",
      "working on chunk 286 at 2022-01-22 18:50:27.184555\n",
      "working on chunk 287 at 2022-01-22 18:50:34.825771\n",
      "working on chunk 288 at 2022-01-22 18:50:42.215066\n",
      "working on chunk 289 at 2022-01-22 18:50:49.465953\n",
      "working on chunk 290 at 2022-01-22 18:50:56.894315\n",
      "working on chunk 291 at 2022-01-22 18:51:04.366895\n",
      "working on chunk 292 at 2022-01-22 18:51:11.618655\n",
      "working on chunk 293 at 2022-01-22 18:51:18.989514\n",
      "working on chunk 294 at 2022-01-22 18:51:26.418092\n",
      "working on chunk 295 at 2022-01-22 18:51:33.660234\n",
      "working on chunk 296 at 2022-01-22 18:51:41.084204\n",
      "working on chunk 297 at 2022-01-22 18:51:48.452460\n",
      "working on chunk 298 at 2022-01-22 18:51:55.684708\n",
      "working on chunk 299 at 2022-01-22 18:52:03.033285\n",
      "working on chunk 300 at 2022-01-22 18:52:10.383251\n",
      "working on chunk 301 at 2022-01-22 18:52:17.630174\n",
      "working on chunk 302 at 2022-01-22 18:52:25.029761\n",
      "working on chunk 303 at 2022-01-22 18:52:32.371062\n",
      "working on chunk 304 at 2022-01-22 18:52:39.619913\n",
      "working on chunk 305 at 2022-01-22 18:52:47.134642\n",
      "working on chunk 306 at 2022-01-22 18:52:54.629449\n",
      "working on chunk 307 at 2022-01-22 18:53:01.870281\n",
      "working on chunk 308 at 2022-01-22 18:53:09.290112\n",
      "working on chunk 309 at 2022-01-22 18:53:16.723040\n",
      "working on chunk 310 at 2022-01-22 18:53:23.924908\n",
      "working on chunk 311 at 2022-01-22 18:53:31.324678\n",
      "working on chunk 312 at 2022-01-22 18:53:38.770313\n",
      "working on chunk 313 at 2022-01-22 18:53:46.015461\n",
      "working on chunk 314 at 2022-01-22 18:53:53.422089\n",
      "working on chunk 315 at 2022-01-22 18:54:00.836506\n",
      "working on chunk 316 at 2022-01-22 18:54:08.245750\n",
      "working on chunk 317 at 2022-01-22 18:54:15.587610\n",
      "working on chunk 318 at 2022-01-22 18:54:22.995124\n",
      "working on chunk 319 at 2022-01-22 18:54:30.183832\n",
      "working on chunk 320 at 2022-01-22 18:54:37.858350\n",
      "working on chunk 321 at 2022-01-22 18:54:45.285029\n",
      "working on chunk 322 at 2022-01-22 18:54:52.537878\n",
      "working on chunk 323 at 2022-01-22 18:54:59.991437\n",
      "working on chunk 324 at 2022-01-22 18:55:07.442910\n",
      "working on chunk 325 at 2022-01-22 18:55:14.634136\n",
      "working on chunk 326 at 2022-01-22 18:55:22.097151\n",
      "working on chunk 327 at 2022-01-22 18:55:29.485574\n",
      "working on chunk 328 at 2022-01-22 18:55:36.736150\n",
      "working on chunk 329 at 2022-01-22 18:55:44.207402\n",
      "working on chunk 330 at 2022-01-22 18:55:51.784599\n",
      "working on chunk 331 at 2022-01-22 18:55:59.057837\n",
      "working on chunk 332 at 2022-01-22 18:56:06.560771\n",
      "working on chunk 333 at 2022-01-22 18:56:14.023695\n",
      "working on chunk 334 at 2022-01-22 18:56:21.239754\n",
      "working on chunk 335 at 2022-01-22 18:56:29.139535\n",
      "working on chunk 336 at 2022-01-22 18:56:36.552319\n",
      "working on chunk 337 at 2022-01-22 18:56:44.160067\n",
      "working on chunk 338 at 2022-01-22 18:56:51.615069\n",
      "working on chunk 339 at 2022-01-22 18:56:59.086262\n",
      "working on chunk 340 at 2022-01-22 18:57:06.423495\n",
      "working on chunk 341 at 2022-01-22 18:57:13.920149\n",
      "working on chunk 342 at 2022-01-22 18:57:21.361258\n",
      "working on chunk 343 at 2022-01-22 18:57:28.600077\n",
      "working on chunk 344 at 2022-01-22 18:57:36.064078\n",
      "working on chunk 345 at 2022-01-22 18:57:43.494547\n",
      "working on chunk 346 at 2022-01-22 18:57:50.734282\n",
      "working on chunk 347 at 2022-01-22 18:57:58.154182\n",
      "working on chunk 348 at 2022-01-22 18:58:05.800610\n",
      "working on chunk 349 at 2022-01-22 18:58:13.074074\n",
      "working on chunk 350 at 2022-01-22 18:58:20.629530\n",
      "complete in 3 seconds\n",
      "skipping 2007.csv\n"
     ]
    }
   ],
   "source": [
    "## Load Flights into database\n",
    "load_flights=False\n",
    "if load_flights:\n",
    "    engine = create_engine(\n",
    "        f'postgresql://{os.getenv(\"DB_USER\")}:{os.getenv(\"DB_PASS\")}@{os.getenv(\"DB_HOST\")}:{os.getenv(\"DB_PORT\")}/{DATABASE_NAME}'\n",
    "    )\n",
    "\n",
    "    #speeds up bulk import\n",
    "    @event.listens_for(engine, \"before_cursor_execute\")\n",
    "    def receive_before_cursor_execute(con, cursor, statement, params, context, executemany):\n",
    "        if executemany:\n",
    "            cursor.fast_executemany=True\n",
    "\n",
    "    for path_ in reversed(FLIGHT_DATA_PATHS):\n",
    "        name = os.path.basename(path_)\n",
    "        year = int(name.rstrip('.csv'))\n",
    "        if year != 2008:\n",
    "            print(f\"skipping {name}\")\n",
    "            break\n",
    "        print(f'uploading {name} at {datetime.datetime.now()}')\n",
    "\n",
    "\n",
    "        for chunk_num, chunk in enumerate(pd.read_csv(\n",
    "            path_, chunksize=20000,encoding='ascii',dtype={\n",
    "                \"Year\": 'int16',  \n",
    "                \"Month\": 'int8',  \n",
    "                \"DayofMonth\": 'int8',  \n",
    "                \"DayOfWeek\": 'int8',  \n",
    "                \"DepTime\": 'float64',\n",
    "                \"CRSDepTime\": 'int16',  \n",
    "                \"ArrTime\": 'float64',\n",
    "                \"CRSArrTime\": 'int16',  \n",
    "                \"UniqueCarrier\": 'string',\n",
    "                \"FlightNum\": 'int16',  \n",
    "                \"ActualElapsedTime\": 'float64',\n",
    "                \"CRSElapsedTime\": 'float64',  \n",
    "                \"AirTime\": 'float64',\n",
    "                \"ArrDelay\": 'float64',\n",
    "                \"DepDelay\": 'float64',\n",
    "                \"Origin\": 'string', \n",
    "                \"Dest\": 'string', \n",
    "                \"Distance\": 'float64',\n",
    "                \"TaxiIn\": 'float64',\n",
    "                \"TaxiOut\": 'float64',\n",
    "                \"Cancelled\": 'int8',  \n",
    "                \"CarrierDelay\": 'float64',\n",
    "                \"WeatherDelay\": 'float64',\n",
    "                \"NASDelay\": 'float64',\n",
    "                \"SecurityDelay\": 'float64',\n",
    "                \"LateAircraftDelay\": 'float64',\n",
    "            })):\n",
    "            # if year == 2004:\n",
    "            #     if chunk_num<104:\n",
    "            #         continue\n",
    "            \n",
    "            # if year == 2007:\n",
    "            #     if chunk_num!=235:\n",
    "            #         print(f'skipping chunk {chunk_num}')\n",
    "            #         continue\n",
    "            task_start = datetime.datetime.now()\n",
    "            print(f'working on chunk {chunk_num} at {task_start}')\n",
    "            chunk = chunk.drop(['TailNum','CancellationCode','Diverted'], axis=1)\n",
    "            chunk.to_sql(\n",
    "                'flights', \n",
    "                engine, \n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "                dtype={\n",
    "                    \"Year\": SMALLINT,  \n",
    "                    \"Month\": SMALLINT,  \n",
    "                    \"DayofMonth\": SMALLINT,  \n",
    "                    \"DayOfWeek\": SMALLINT,  \n",
    "                    \"DepTime\": SMALLINT,\n",
    "                    \"CRSDepTime\": SMALLINT,  \n",
    "                    \"ArrTime\": SMALLINT,\n",
    "                    \"CRSArrTime\": SMALLINT,  \n",
    "                    \"UniqueCarrier\": VARCHAR,\n",
    "                    \"FlightNum\": SMALLINT,  \n",
    "                    \"ActualElapsedTime\": SMALLINT,\n",
    "                    \"CRSElapsedTime\": SMALLINT,  \n",
    "                    \"AirTime\": SMALLINT,\n",
    "                    \"ArrDelay\": SMALLINT,\n",
    "                    \"DepDelay\": SMALLINT,\n",
    "                    \"Origin\": VARCHAR, \n",
    "                    \"Dest\": VARCHAR, \n",
    "                    \"Distance\": SMALLINT,\n",
    "                    \"TaxiIn\": SMALLINT,\n",
    "                    \"TaxiOut\": SMALLINT,\n",
    "                    \"Cancelled\": BOOLEAN,  \n",
    "                    \"CarrierDelay\": SMALLINT,\n",
    "                    \"WeatherDelay\": SMALLINT,\n",
    "                    \"NASDelay\": SMALLINT,\n",
    "                    \"SecurityDelay\": SMALLINT,\n",
    "                    \"LateAircraftDelay\": SMALLINT,\n",
    "                    })\n",
    "        complete_in = datetime.datetime.now()-task_start\n",
    "        print(f'complete in {complete_in.seconds} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 1:\tWhat percentage of flights were canceled each year from 1999 to 2003?\n",
    "*I'd normally do this with a simple query of total flights cancelled and total flights, then calculate the percentage in python, but the prompt is asking for these to be done in SQL, so i'll do that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection opened ...\n",
      "[(739611, 28938610, 2.55579310823844)]\n",
      "... connection closed.\n"
     ]
    }
   ],
   "source": [
    "conn = DBConn()\n",
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT \n",
    "    count_cancelled,\n",
    "    total,\n",
    "    (count_cancelled/total::FLOAT)*100 perc_cancelled\n",
    "FROM (\n",
    "    SELECT \n",
    "        Count(*) total, \n",
    "        SUM(CASE WHEN \"Cancelled\" THEN 1 ELSE 0 END) count_cancelled\n",
    "    FROM flights\n",
    "    WHERE \"Year\" >= 1999 AND \"Year\" <= 2003\n",
    ") x;\n",
    "\"\"\"\n",
    "ans_1 = conn.exec(q)\n",
    "print(ans_1)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Cancelled Flights | Total Flights | Percentage Cancelled |\n",
    "|-------------------|---------------|-------------------|\n",
    "| 739611            | 28938610      | 2.55579310823844     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2: On which day of the week in 2007 were you most likely to arrive on time flying from MCO to IAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection opened ...\n",
      "[(134, 377, 35.5437665782493), (130, 346, 37.5722543352601), (161, 351, 45.8689458689459), (184, 390, 47.1794871794872), (175, 398, 43.9698492462312), (140, 377, 37.1352785145889), (152, 405, 37.5308641975309)]\n",
      "... connection closed.\n"
     ]
    }
   ],
   "source": [
    "conn = DBConn()\n",
    "q = \"\"\"\n",
    "--sql\n",
    "SELECT\n",
    "\ton_time_count,\n",
    "\ttotal_flights,\n",
    "\t(on_time_count/total_flights::FLOAT)*100 percent_ontime\n",
    "FROM\n",
    "(SELECT \n",
    "\tSUM(CASE WHEN \"CRSArrTime\" <= \"ArrTime\" THEN 1 ELSE 0 END) on_time_count,\n",
    "\tCOUNT(*) total_flights\n",
    "FROM flights\n",
    "WHERE \"Year\" = 2007 AND \"Origin\" = 'MCO' AND \"Dest\" = 'IAH'\n",
    "GROUP BY \"DayOfWeek\"\n",
    ") x;\n",
    "\"\"\"\n",
    "ans_2 = conn.exec(q)\n",
    "print(ans_2)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Week Day   | On Time Flights   | Total Flights | Percentage On time    |\n",
    "|----       |-------------------|---------------|-------------------    |\n",
    "|1 (Mon)    |134\t            | 377       \t| 35.5437665782493      |\n",
    "|2 (Tue)    |130\t            | 346       \t| 37.5722543352601      |\n",
    "|3 (Wed)    |161\t            | 351       \t| 45.8689458689459      |\n",
    "|**4 (Thu)**|**184**\t        | **390**      \t| **47.1794871794872**  |\n",
    "|5 (Fri)    |175\t            | 398       \t| 43.9698492462312      |\n",
    "|6 (Sat)    |140\t            | 377       \t| 37.1352785145889      |\n",
    "|7 (sun)    |152\t            | 405       \t| 37.5308641975309      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3.\tWhich 10 flights (airline, flight number, origin city, destination city, and date) had the latest actual vs. scheduled arrival in 2004?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4.\tWhich 10 flights (airline, flight number, origin city, destination city, and date) had the latest actual vs. scheduled arrival in 2004?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 5.\tFor the years 2002 to 2005, what is the ratio of carrier delay to elapsed travel time for each airline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 6.\tWhat airline spent the most and least average time taxiing (in and out) at JFK in 2006?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 7.\tWhat were the top 10 routes (origin and destination city names and airport codes) most likely to have a weather delay of over 10 minutes in December 2005?\n",
    "*Only consider routes with at least 20 flights that month.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 8.\tFlying Southwest, what is the year-over-year change in on-time travel rate from 2000 to 2007?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 9.\tWhat was the month-to-date on-time arrival rate for United for each date in September 2005?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62fceacedb450e62e0875c801c359dca848f3b893084d28dc80946ee182fde9c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('TME-Local': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
